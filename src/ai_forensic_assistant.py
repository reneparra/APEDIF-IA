#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Asistente IA Forense Mejorado - IntegraciÃ³n con Llama 3.1 8B
Proporciona interpretaciÃ³n inteligente de anÃ¡lisis forenses usando IA local
"""

import json
import os
import subprocess
import datetime
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
import time
import threading

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('C:/ForensicAI/logs/ai_assistant.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedForensicAIAssistant:
    """Asistente IA forense mejorado con Llama 3.1 8B"""
    
    def __init__(self, model_name: str = "llama3.1:8b"):
        self.model_name = model_name
        self.is_ollama_available = False
        self.conversation_history = []
        self.current_case_context = None
        
        # Prompts especializados en forense
        self.system_prompts = {
            'forensic_expert': """Eres un experto senior en informÃ¡tica forense con 15 aÃ±os de experiencia en investigaciones digitales. Tu especialidad incluye:

- AnÃ¡lisis de evidencia digital en casos corporativos
- InterpretaciÃ³n de patrones de comportamiento sospechoso
- EvaluaciÃ³n de riesgo y amenazas internas
- GeneraciÃ³n de reportes tÃ©cnicos para tribunales
- Recomendaciones de investigaciÃ³n forense

INSTRUCCIONES IMPORTANTES:
1. Responde SIEMPRE en espaÃ±ol de manera profesional y tÃ©cnica
2. Basa tus anÃ¡lisis en la evidencia digital proporcionada
3. Proporciona recomendaciones especÃ­ficas y accionables
4. Usa terminologÃ­a forense apropiada
5. MantÃ©n objetividad cientÃ­fica en tus conclusiones
6. Considera aspectos legales y de cumplimiento normativo

Contexto: EstÃ¡s analizando evidencia digital en una investigaciÃ³n corporativa.""",

            'executive_summary': """Eres un consultor forense senior que debe generar resÃºmenes ejecutivos para directivos y equipos legales. 

Tu tarea: Crear un resumen claro, conciso y profesional que explique:
- Los hallazgos mÃ¡s importantes
- El nivel de riesgo para la organizaciÃ³n
- Las implicaciones legales y de negocio
- Las acciones inmediatas recomendadas

Responde en espaÃ±ol con un lenguaje profesional pero accesible para no-tÃ©cnicos.""",

            'technical_analysis': """Eres un analista forense tÃ©cnico especializado en interpretaciÃ³n detallada de evidencia digital.

EnfÃ³cate en:
- AnÃ¡lisis tÃ©cnico profundo de los datos
- CorrelaciÃ³n de eventos y patrones
- MetodologÃ­as forenses aplicadas
- Validez tÃ©cnica de la evidencia
- Recomendaciones tÃ©cnicas especÃ­ficas

Responde en espaÃ±ol con terminologÃ­a tÃ©cnica precisa.""",

            'legal_compliance': """Eres un especialista en cumplimiento legal y normativo en investigaciones forenses digitales.

Considera:
- Aspectos de protecciÃ³n de datos (GDPR, CCPA)
- Cadena de custodia de evidencia
- Admisibilidad legal de la evidencia
- Recomendaciones de compliance
- Riesgos legales y regulatorios

Responde en espaÃ±ol con enfoque en aspectos legales y normativos."""
        }
        
        # Verificar disponibilidad de Ollama
        self._check_ollama_availability()

    def _check_ollama_availability(self) -> bool:
        """Verifica si Ollama estÃ¡ disponible y el modelo estÃ¡ descargado"""
        try:
            # Verificar que Ollama estÃ© instalado
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode != 0:
                logger.error("Ollama no estÃ¡ instalado o no responde")
                return False
            
            # Verificar que el modelo estÃ© disponible
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            if self.model_name not in result.stdout:
                logger.warning(f"Modelo {self.model_name} no encontrado. Intentando descargarlo...")
                self._download_model()
            
            self.is_ollama_available = True
            logger.info(f"Ollama y modelo {self.model_name} disponibles")
            return True
            
        except subprocess.TimeoutExpired:
            logger.error("Timeout al verificar Ollama")
            return False
        except FileNotFoundError:
            logger.error("Ollama no estÃ¡ instalado en el sistema")
            return False
        except Exception as e:
            logger.error(f"Error verificando Ollama: {e}")
            return False

    def _download_model(self) -> bool:
        """Descarga el modelo Llama 3.1 8B si no estÃ¡ disponible"""
        try:
            logger.info(f"Descargando modelo {self.model_name}... (esto puede tomar varios minutos)")
            
            # Ejecutar descarga en hilo separado para no bloquear
            def download_thread():
                subprocess.run(['ollama', 'pull', self.model_name], 
                             capture_output=True, text=True, timeout=1800)  # 30 min timeout
            
            thread = threading.Thread(target=download_thread)
            thread.start()
            thread.join(timeout=1800)  # 30 minutos mÃ¡ximo
            
            if thread.is_alive():
                logger.error("Timeout descargando modelo")
                return False
            
            logger.info(f"Modelo {self.model_name} descargado exitosamente")
            return True
            
        except Exception as e:
            logger.error(f"Error descargando modelo: {e}")
            return False

    def _call_ollama(self, prompt: str, system_prompt: str = None, 
                    temperature: float = 0.1, max_tokens: int = 4096) -> str:
        """Realiza llamada a Ollama con manejo de errores mejorado"""
        if not self.is_ollama_available:
            return "Error: IA no disponible. Verificar instalaciÃ³n de Ollama."
        
        try:
            # Preparar el prompt completo
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\nPregunta del usuario: {prompt}"
            
            # Configurar parÃ¡metros del modelo
            model_params = {
                'temperature': temperature,
                'num_predict': max_tokens,
                'top_k': 40,
                'top_p': 0.9
            }
            
            # Preparar comando
            cmd = ['ollama', 'run', self.model_name]
            
            # Ejecutar consulta con timeout
            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                errors='replace'
            )
            
            stdout, stderr = process.communicate(input=full_prompt, timeout=120)
            
            if process.returncode != 0:
                logger.error(f"Error en Ollama: {stderr}")
                return f"Error en IA: {stderr[:200]}..."
            
            # Limpiar respuesta
            response = stdout.strip()
            if not response:
                return "Error: La IA no generÃ³ respuesta."
            
            # Registrar en historial
            self.conversation_history.append({
                'timestamp': datetime.datetime.now().isoformat(),
                'prompt': prompt,
                'response': response,
                'model': self.model_name
            })
            
            logger.info(f"Respuesta de IA generada exitosamente ({len(response)} caracteres)")
            return response
            
        except subprocess.TimeoutExpired:
            logger.error("Timeout en consulta a IA")
            return "Error: Timeout - La consulta tardÃ³ demasiado tiempo."
        except Exception as e:
            logger.error(f"Error en llamada a IA: {e}")
            return f"Error inesperado en IA: {str(e)[:200]}..."

    def load_case_context(self, case_file_path: str) -> bool:
        """Carga el contexto de un caso para anÃ¡lisis"""
        try:
            with open(case_file_path, 'r', encoding='utf-8') as f:
                self.current_case_context = json.load(f)
            
            logger.info(f"Contexto de caso cargado: {self.current_case_context.get('case_id', 'Unknown')}")
            return True
            
        except Exception as e:
            logger.error(f"Error cargando contexto del caso: {e}")
            return False

    def generate_executive_summary(self, analysis_result: Dict[str, Any]) -> str:
        """Genera resumen ejecutivo usando IA"""
        logger.info("Generando resumen ejecutivo con IA...")
        
        # Preparar datos del anÃ¡lisis para la IA
        case_info = analysis_result.get('case_info', {})
        summary = analysis_result.get('summary', {})
        risk_assessment = analysis_result.get('risk_assessment', {})
        
        prompt = f"""
ANÃLISIS FORENSE DIGITAL - RESUMEN EJECUTIVO

INFORMACIÃ“N DEL CASO:
- ID del Caso: {case_info.get('case_id')}
- Tipo de InvestigaciÃ³n: {case_info.get('case_type', '').replace('_', ' ').title()}
- Sospechoso: {case_info.get('suspect_name')}
- Fecha de AnÃ¡lisis: {case_info.get('analysis_timestamp')}

ESTADÃSTICAS CLAVE:
- Archivos Analizados: {summary.get('total_files_analyzed', 0)}
- Archivos Sospechosos: {summary.get('suspicious_files_found', 0)}
- Conexiones de Red: {summary.get('total_network_connections', 0)}
- Conexiones Sospechosas: {summary.get('suspicious_connections_found', 0)}

EVALUACIÃ“N DE RIESGO:
- Nivel de Riesgo: {risk_assessment.get('risk_level')}
- PuntuaciÃ³n: {risk_assessment.get('overall_risk_score')}/100
- Confianza: {risk_assessment.get('confidence_level')}

FACTORES DE RIESGO DETECTADOS:
"""
        
        for i, factor in enumerate(risk_assessment.get('risk_factors', []), 1):
            prompt += f"{i}. {factor}\n"
        
        prompt += """
INSTRUCCIONES:
Genera un resumen ejecutivo profesional para directivos que incluya:
1. EvaluaciÃ³n del riesgo para la organizaciÃ³n
2. Principales hallazgos crÃ­ticos
3. Implicaciones de negocio y legales
4. Recomendaciones inmediatas prioritarias
5. PrÃ³ximos pasos sugeridos

El resumen debe ser claro, conciso y enfocado en decisiones ejecutivas.
        """
        
        return self._call_ollama(prompt, self.system_prompts['executive_summary'])

    def analyze_temporal_patterns(self, analysis_result: Dict[str, Any]) -> str:
        """Analiza patrones temporales usando IA"""
        logger.info("Analizando patrones temporales con IA...")
        
        file_analysis = analysis_result.get('file_analysis', {})
        network_analysis = analysis_result.get('network_analysis', {})
        
        prompt = f"""
ANÃLISIS DE PATRONES TEMPORALES

ACTIVIDAD DE ARCHIVOS:
- Eventos fuera de horario: {file_analysis.get('after_hours_activity', 0)}
- DistribuciÃ³n horaria: {file_analysis.get('hourly_distribution', {})}

ACTIVIDAD DE RED:
- Conexiones fuera de horario: {network_analysis.get('after_hours_connections', 0)}
- DistribuciÃ³n horaria de red: {network_analysis.get('hourly_distribution', {})}

INSTRUCCIONES:
Analiza estos patrones temporales y proporciona:
1. InterpretaciÃ³n del comportamiento temporal anÃ³malo
2. Significancia forense de la actividad fuera de horario
3. CorrelaciÃ³n entre actividad de archivos y red
4. Posibles explicaciones para estos patrones
5. Recomendaciones para investigaciÃ³n adicional

EnfÃ³cate en el aspecto forense y las implicaciones de estos patrones temporales.
        """
        
        return self._call_ollama(prompt, self.system_prompts['technical_analysis'])

    def analyze_network_behavior(self, analysis_result: Dict[str, Any]) -> str:
        """Analiza comportamiento de red usando IA"""
        logger.info("Analizando comportamiento de red con IA...")
        
        network_analysis = analysis_result.get('network_analysis', {})
        
        prompt = f"""
ANÃLISIS DE COMPORTAMIENTO DE RED

ESTADÃSTICAS DE CONEXIONES:
- Total de conexiones: {network_analysis.get('total_connections', 0)}
- Conexiones sospechosas: {network_analysis.get('suspicious_connections', 0)}
- Transferencias grandes: {network_analysis.get('large_transfers_count', 0)}
- Servicios en la nube: {network_analysis.get('cloud_services_count', 0)}

DISTRIBUCIÃ“N DE DESTINOS:
{network_analysis.get('destinations_distribution', {})}

DISTRIBUCIÃ“N DE PUERTOS:
{network_analysis.get('ports_distribution', {})}

DISTRIBUCIÃ“N DE PROTOCOLOS:
{network_analysis.get('protocols_distribution', {})}

INSTRUCCIONES:
Analiza este comportamiento de red y proporciona:
1. EvaluaciÃ³n de amenazas en las conexiones detectadas
2. AnÃ¡lisis de patrones de exfiltraciÃ³n de datos
3. IdentificaciÃ³n de actividad maliciosa potencial
4. EvaluaciÃ³n del uso de servicios en la nube
5. Recomendaciones tÃ©cnicas especÃ­ficas para la investigaciÃ³n

EnfÃ³cate en aspectos de ciberseguridad e investigaciÃ³n forense.
        """
        
        return self._call_ollama(prompt, self.system_prompts['technical_analysis'])

    def generate_recommendations(self, analysis_result: Dict[str, Any]) -> str:
        """Genera recomendaciones especÃ­ficas usando IA"""
        logger.info("Generando recomendaciones con IA...")
        
        risk_assessment = analysis_result.get('risk_assessment', {})
        threat_categories = risk_assessment.get('threat_categories', {})
        
        prompt = f"""
GENERACIÃ“N DE RECOMENDACIONES FORENSES

NIVEL DE RIESGO: {risk_assessment.get('risk_level')} ({risk_assessment.get('overall_risk_score')}/100)

CATEGORÃAS DE AMENAZAS DETECTADAS:
- ExfiltraciÃ³n de datos: {threat_categories.get('data_exfiltration', 0)} indicadores
- DestrucciÃ³n de evidencia: {threat_categories.get('evidence_destruction', 0)} indicadores
- Acceso no autorizado: {threat_categories.get('unauthorized_access', 0)} indicadores
- Amenaza interna: {threat_categories.get('insider_threat', 0)} indicadores

FACTORES DE RIESGO:
"""
        
        for factor in risk_assessment.get('risk_factors', []):
            prompt += f"- {factor}\n"
        
        prompt += """
INSTRUCCIONES:
Genera recomendaciones especÃ­ficas y priorizadas que incluyan:

ACCIONES INMEDIATAS (prÃ³ximas 24-48 horas):
1. Medidas de preservaciÃ³n de evidencia
2. Acciones de contenciÃ³n de riesgos
3. Pasos crÃ­ticos de investigaciÃ³n

ACCIONES A MEDIANO PLAZO (prÃ³ximas 1-2 semanas):
1. InvestigaciÃ³n forense detallada
2. Entrevistas y procedimientos legales
3. AnÃ¡lisis tÃ©cnico profundo

ACCIONES PREVENTIVAS A LARGO PLAZO:
1. Mejoras en controles de seguridad
2. PolÃ­ticas y procedimientos
3. Monitoreo y detecciÃ³n

Cada recomendaciÃ³n debe ser especÃ­fica, accionable y justificada tÃ©cnicamente.
        """
        
        return self._call_ollama(prompt, self.system_prompts['forensic_expert'])

    def generate_narrative_report(self, analysis_result: Dict[str, Any]) -> str:
        """Genera reporte narrativo completo usando IA"""
        logger.info("Generando reporte narrativo completo con IA...")
        
        case_info = analysis_result.get('case_info', {})
        summary = analysis_result.get('summary', {})
        risk_assessment = analysis_result.get('risk_assessment', {})
        
        prompt = f"""
GENERACIÃ“N DE REPORTE NARRATIVO FORENSE

CASO: {case_info.get('case_id')}
TIPO: {case_info.get('case_type', '').replace('_', ' ').title()}
SOSPECHOSO: {case_info.get('suspect_name')}

EVIDENCIA ANALIZADA:
- {summary.get('total_files_analyzed', 0)} archivos digitales
- {summary.get('total_network_connections', 0)} conexiones de red
- Nivel de riesgo: {risk_assessment.get('risk_level')} ({risk_assessment.get('overall_risk_score')}/100)

HALLAZGOS PRINCIPALES:
- {summary.get('suspicious_files_found', 0)} archivos sospechosos identificados
- {summary.get('suspicious_connections_found', 0)} conexiones de red anÃ³malas
- Confianza en el anÃ¡lisis: {risk_assessment.get('confidence_level')}

INSTRUCCIONES:
Genera un reporte narrativo profesional estilo pericial que incluya:

1. INTRODUCCIÃ“N Y METODOLOGÃA
2. DESCRIPCIÃ“N DE LA EVIDENCIA ANALIZADA
3. HALLAZGOS TÃ‰CNICOS DETALLADOS
4. ANÃLISIS E INTERPRETACIÃ“N DE EVIDENCIA
5. CONCLUSIONES Y OPINIÃ“N TÃ‰CNICA
6. LIMITACIONES DEL ANÃLISIS

El reporte debe ser:
- TÃ©cnicamente preciso y detallado
- Apropiado para uso judicial
- Redactado en espaÃ±ol formal
- Estructurado y profesional
- Basado Ãºnicamente en la evidencia analizada

Incluye referencias a metodologÃ­as forenses estÃ¡ndar y mantÃ©n objetividad cientÃ­fica.
        """
        
        return self._call_ollama(prompt, self.system_prompts['legal_compliance'], temperature=0.05)

    def answer_custom_question(self, question: str, analysis_result: Dict[str, Any] = None) -> str:
        """Responde preguntas personalizadas sobre el caso"""
        logger.info(f"Respondiendo pregunta personalizada: {question[:50]}...")
        
        context = ""
        if analysis_result:
            case_info = analysis_result.get('case_info', {})
            summary = analysis_result.get('summary', {})
            
            context = f"""
CONTEXTO DEL CASO ACTUAL:
- Caso: {case_info.get('case_id')}
- Sospechoso: {case_info.get('suspect_name')}
- Archivos analizados: {summary.get('total_files_analyzed', 0)}
- Archivos sospechosos: {summary.get('suspicious_files_found', 0)}
- Conexiones de red: {summary.get('total_network_connections', 0)}
- Nivel de riesgo: {analysis_result.get('risk_assessment', {}).get('risk_level')}

"""
        
        prompt = f"""
{context}

PREGUNTA DEL USUARIO: {question}

INSTRUCCIONES:
Responde la pregunta basÃ¡ndote en:
1. Tu experiencia en informÃ¡tica forense
2. El contexto del caso actual (si estÃ¡ disponible)
3. Mejores prÃ¡cticas en investigaciÃ³n digital
4. Consideraciones legales y tÃ©cnicas relevantes

Proporciona una respuesta detallada, tÃ©cnica y profesional en espaÃ±ol.
        """
        
        return self._call_ollama(prompt, self.system_prompts['forensic_expert'])

    def get_case_insights(self, analysis_result: Dict[str, Any]) -> Dict[str, str]:
        """Obtiene insights completos del caso usando IA"""
        logger.info("Generando insights completos del caso...")
        
        insights = {}
        
        try:
            # Generar diferentes tipos de anÃ¡lisis
            insights['executive_summary'] = self.generate_executive_summary(analysis_result)
            insights['temporal_analysis'] = self.analyze_temporal_patterns(analysis_result)
            insights['network_analysis'] = self.analyze_network_behavior(analysis_result)
            insights['recommendations'] = self.generate_recommendations(analysis_result)
            
            logger.info("Insights completos generados exitosamente")
            
        except Exception as e:
            logger.error(f"Error generando insights: {e}")
            insights['error'] = f"Error generando anÃ¡lisis IA: {str(e)}"
        
        return insights

    def save_ai_analysis(self, insights: Dict[str, str], case_id: str, 
                        output_dir: str = "C:/ForensicAI/reportes") -> str:
        """Guarda el anÃ¡lisis de IA en archivo"""
        os.makedirs(output_dir, exist_ok=True)
        
        filename = f"{case_id}_ai_analysis.json"
        filepath = os.path.join(output_dir, filename)
        
        ai_report = {
            'case_id': case_id,
            'analysis_timestamp': datetime.datetime.now().isoformat(),
            'model_used': self.model_name,
            'insights': insights,
            'conversation_history': self.conversation_history
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(ai_report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"AnÃ¡lisis de IA guardado en: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error guardando anÃ¡lisis de IA: {e}")
            raise

def main():
    """FunciÃ³n principal para demostrar el asistente IA"""
    print("ğŸ¤– ASISTENTE IA FORENSE MEJORADO")
    print("=" * 50)
    
    # Inicializar asistente
    assistant = EnhancedForensicAIAssistant()
    
    if not assistant.is_ollama_available:
        print("âŒ Ollama no estÃ¡ disponible.")
        print("   Instala Ollama y el modelo llama3.1:8b")
        return
    
    print(f"âœ… IA inicializada con modelo: {assistant.model_name}")
    
    # Buscar anÃ¡lisis disponibles
    reports_dir = "C:/ForensicAI/reportes"
    
    if not os.path.exists(reports_dir):
        print("âŒ No se encontraron reportes para analizar.")
        print("   Ejecuta primero: python forensic_analyzer.py")
        return
    
    analysis_files = [f for f in os.listdir(reports_dir) if f.endswith('_analysis_report.json')]
    
    if not analysis_files:
        print("âŒ No se encontraron reportes de anÃ¡lisis.")
        return
    
    # Cargar el primer anÃ¡lisis disponible
    analysis_file = os.path.join(reports_dir, analysis_files[0])
    
    print(f"ğŸ“‹ Analizando con IA: {analysis_files[0]}")
    print("-" * 50)
    
    try:
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_result = json.load(f)
        
        case_id = analysis_result['case_info']['case_id']
        
        print(f"ğŸ”„ Generando anÃ¡lisis IA para caso: {case_id}")
        
        # Generar insights completos
        insights = assistant.get_case_insights(analysis_result)
        
        # Mostrar resumen ejecutivo
        if 'executive_summary' in insights:
            print(f"\nğŸ“Š RESUMEN EJECUTIVO (IA):")
            print("-" * 50)
            print(insights['executive_summary'][:500] + "..." if len(insights['executive_summary']) > 500 else insights['executive_summary'])
        
        # Guardar anÃ¡lisis completo
        ai_report_file = assistant.save_ai_analysis(insights, case_id)
        
        print(f"\nğŸ’¾ AnÃ¡lisis IA completo guardado en:")
        print(f"   {ai_report_file}")
        
        # Ofrecer consulta interactiva
        print(f"\nğŸ’¬ Â¿Deseas hacer una consulta personalizada? (s/n): ", end="")
        response = input().lower().strip()
        
        if response in ['s', 'si', 'sÃ­', 'y', 'yes']:
            print(f"\nğŸ¤– Modo consulta interactiva activado")
            print(f"Escribe 'salir' para terminar\n")
            
            while True:
                question = input("â“ Tu pregunta: ").strip()
                
                if question.lower() in ['salir', 'exit', 'quit']:
                    break
                
                if not question:
                    continue
                
                print(f"\nğŸ”„ Consultando IA...")
                answer = assistant.answer_custom_question(question, analysis_result)
                print(f"\nğŸ¤– Respuesta:")
                print(f"{answer}\n")
                print("-" * 50)
        
        print(f"\nğŸ‰ Â¡AnÃ¡lisis IA completado exitosamente!")
        
    except Exception as e:
        print(f"âŒ Error durante el anÃ¡lisis IA: {e}")
        logger.error(f"Error en main: {e}")

if __name__ == "__main__":
    main()
    input("\nPresiona Enter para continuar...")